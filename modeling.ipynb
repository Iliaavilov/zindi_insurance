{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neptunecontrib.monitoring.optuna as optuna_utils\n",
    "import optuna\n",
    "import numpy as np\n",
    "import neptune\n",
    "import gc\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(iliaavilov/Zindi-insurance)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## инициализируем проект в нептуне\n",
    "neptune.init('iliaavilov/Zindi-insurance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренировочные данные\n",
    "train = pd.read_csv('train_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(data, column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Кодировка колонки датафрейма через LabelEncoder из sklearn\n",
    "\n",
    "    ----------\n",
    "    data_train - датафрейм\n",
    "    column - колонка датафрейма, которую надо закодировать\n",
    "    ----------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(data[column].values)\n",
    "    data[column] = le.transform(data[column].values)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = encoding(train, 'variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кросс валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = train[['ID']].drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv(data, n_splits, random_state):\n",
    "    \n",
    "    \"\"\"\n",
    "    Разбивает датафрейм на несколько фолдов CV с помощью KFold из sklearn\n",
    "\n",
    "    ----------\n",
    "    data - датафрейм\n",
    "    n_splits - количество фолдов в CV\n",
    "    random_state - сид рандомного состояния для KFold из sklearn\n",
    "    ----------\n",
    "\n",
    "    \"\"\"    \n",
    "    \n",
    "    kf = KFold(n_splits = n_splits, shuffle = True, random_state = random_state)\n",
    "    cv = list(kf.split(data))\n",
    "    cv_users = []\n",
    "    \n",
    "    for cur_cv in cv:\n",
    "        train = data.iloc[cur_cv[0]]['ID'].values\n",
    "        test = data.iloc[cur_cv[1]]['ID'].values\n",
    "        cv_users.append((train, test))\n",
    "        \n",
    "    return(cv_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_users = split_cv(users, n_splits= 3, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = []\n",
    "\n",
    "for fold in cv_users:\n",
    "    cv.append((\n",
    "        train[train['ID'].isin(fold[0])].index.values, \n",
    "        train[train['ID'].isin(fold[1])].index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('ID', axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка разных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_training(X_train, y_train, X_test, y_test, params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Тренирует lgbm модель и возвращает log loss модели на тестовой выборке\n",
    "\n",
    "    ----------\n",
    "    X_train - тренировочные данные\n",
    "    y_train - тренировочный target\n",
    "    X_test - тестовые данные\n",
    "    y_test - тестовый target\n",
    "    params - параметры модели\n",
    "    ----------\n",
    "\n",
    "    \"\"\"        \n",
    "\n",
    "    train_data = lgb.Dataset(X_train, y_train)\n",
    "    model_trial = lgb.train(params, train_data)\n",
    "    predictions = model_trial.predict(X_test)\n",
    "    cur_score = log_loss(y_test, np.array([1- predictions, predictions]).T)\n",
    "    \n",
    "    return(cur_score)\n",
    "\n",
    "\n",
    "\n",
    "def xgb_training(X_train, y_train, X_test, y_test, params):\n",
    "\n",
    "    \"\"\"\n",
    "    Тренирует xgboost модель и возвращает log loss модели на тестовой выборке\n",
    "\n",
    "    ----------\n",
    "    X_train - тренировочные данные\n",
    "    y_train - тренировочный target\n",
    "    X_test - тестовые данные\n",
    "    y_test - тестовый target\n",
    "    params - параметры модели\n",
    "    ----------\n",
    "\n",
    "    \"\"\"  \n",
    "    \n",
    "    train_data = xgb.DMatrix(X_train, label = y_train)\n",
    "    test_data = xgb.DMatrix(X_test)\n",
    "    model_trial = xgb.train(params = params, dtrain = train_data, num_boost_round = params['num_boost_round'])\n",
    "    predictions = model_trial.predict(test_data)\n",
    "    cur_score = log_loss(y_test, np.array([1- predictions, predictions]).T)\n",
    "    \n",
    "    return(cur_score)\n",
    "\n",
    "\n",
    "\n",
    "def sklearn_models_training(X_train, y_train, X_test, y_test, params):\n",
    "\n",
    "    \"\"\"\n",
    "    Тренирует sklearn модель (или любую другую модель с таким же функционалом) и возвращает log loss \n",
    "    модели на тестовой выборке\n",
    "\n",
    "    ----------\n",
    "    X_train - тренировочные данные\n",
    "    y_train - тренировочный target\n",
    "    X_test - тестовые данные\n",
    "    y_test - тестовый target\n",
    "    params - параметры модели\n",
    "    ----------\n",
    "\n",
    "    \"\"\"  \n",
    "    \n",
    "    model_trial = clone(model)\n",
    "    model_trial.set_params(**params)\n",
    "    model_trial.fit(X_train, y_train)\n",
    "    predictions = model_trial.predict_proba(X_test)\n",
    "    cur_score = log_loss(y_test, y_train)\n",
    "    \n",
    "    return(cur_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция, которую optuna будет пытаться минимизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model, train, cv, random_state):\n",
    "\n",
    "    \"\"\"\n",
    "    Objective функция для study из optuna. Возвращает очки на каждом этапе подбора гиперпараметров\n",
    "\n",
    "    ----------\n",
    "    trial - объект обучения из optuna\n",
    "    model - модель (xgboost/lgbm/sklean-подобная модель)\n",
    "    train - тренировочный датасет и таргет\n",
    "    random_state - сид рандомного состояния для KFold из sklearn\n",
    "    ----------\n",
    "\n",
    "    \"\"\"  \n",
    "    \n",
    "    ## Множество параметров моделей\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2500),\n",
    "        'random_state': random_state,\n",
    "        'categorical_feature': [train.columns.get_loc(cat_col) for cat_col in \n",
    "                                ['sex', 'marital_status', 'branch_code', 'occupation_code',\n",
    "                                 'occupation_category_code', 'variable', 'P5DA',\n",
    "                                 'RIBP', '8NN1', '7POT', '66FJ', 'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9',\n",
    "                                 'N2MW', 'AHXO', 'BSTQ', 'FM3X', 'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW',\n",
    "                                 'GHYX', 'ECY3']],\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.5),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0)\n",
    "    }\n",
    "    \n",
    "    ## Список результатов для разных фолдов\n",
    "    score = []\n",
    "    \n",
    "    for fold in cv:\n",
    "        \n",
    "        ## Выбираем X и y\n",
    "        X_train = train.iloc[fold[0], :].drop('presence', axis = 'columns')\n",
    "        y_train = train.iloc[fold[0], :]['presence'].values\n",
    "        X_test = train.iloc[fold[1], :].drop('presence', axis = 'columns')\n",
    "        y_test = train.iloc[fold[1], :]['presence'].values\n",
    "        \n",
    "        \n",
    "        ## В зависимости от заданной модели трренируем соответствующую модель\n",
    "        if model == 'lgbm':\n",
    "            cur_score = lgbm_training(X_train, y_train, X_test, y_test, params)\n",
    "            score.append(cur_score)\n",
    "            \n",
    "        elif model == 'xgb':\n",
    "            cur_score = xgb_training(X_train, y_train, X_test, y_test, params)\n",
    "            score.append(cur_score)\n",
    "            \n",
    "        else:\n",
    "            cur_score = sklearn_models_training(X_train, y_train, X_test, y_test, params)\n",
    "            score.append(cur_score)\n",
    "        \n",
    "        \n",
    "        print(cur_score)\n",
    "    \n",
    "    ## Логируем список результатов для каждого фолда\n",
    "    neptune.log_text('CV scores', str(score))\n",
    "    \n",
    "    ## Возвращаем средний результат по фолдам, который будем минимизировать\n",
    "    return(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train, cv, model, random_state, n_trials, tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Подбирает гиперрпараметры для заданной модели\n",
    "\n",
    "    ----------\n",
    "    train - тренировочный датасет и таргет\n",
    "    cv - список списков с индексами для трренировчной/тестовой части вида [[train_index0, test_index0], ...]\n",
    "    model - модель (xgboost/lgbm/sklean-подобная модель)\n",
    "    random_state - сид рандомного состояния для KFold из sklearn\n",
    "    n_trials - количество попыток найти лучшие гиперпараметры\n",
    "    tags - тэги для записи эксперимента в optuna\n",
    "    ----------\n",
    "\n",
    "    \"\"\"  \n",
    "    \n",
    "    ## Делаем переменную study глобальной, чтобы в случае незапланированной остановки можно было продолжить подбор \n",
    "    ## параметров с того момента, на котором остановились\n",
    "    global study\n",
    "    \n",
    "    ## Создаем эксперимент в neptune\n",
    "    if type(model) == str:\n",
    "        current_experiment = neptune.create_experiment(model, tags = tags)\n",
    "    else:\n",
    "        current_experiment = neptune.create_experiment(type(model).__name__, tags = tags)\n",
    "        \n",
    "    ## Выбираем стандартный сэмплер для подборки параметров\n",
    "    sampler = optuna.samplers.TPESampler(seed = random_state)\n",
    "    \n",
    "    ## Задаем объект study, с помощью которого будем МИНИМИЗИРОВАТЬ ошибку\n",
    "    study = optuna.create_study(sampler = sampler, direction = 'minimize')\n",
    "    \n",
    "    ## минимизируем ошибку\n",
    "    study.optimize(lambda trial: objective(trial, model, train, cv, random_state), \n",
    "                   n_trials = n_trials, callbacks = [optuna_utils.NeptuneCallback()]\n",
    "                  )\n",
    "    \n",
    "    ## Логируем объект study\n",
    "    optuna_utils.log_study(study)\n",
    "    \n",
    "    neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/iliaavilov/Zindi-insurance/e/ZIN-922\n",
      "0.03200842000695025\n",
      "0.03277893643365938\n",
      "0.03168116624582877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-07 21:57:56,365] Finished trial#0 with value: 0.0321561742288128 with parameters: {'n_estimators': 910, 'num_leaves': 176, 'learning_rate': 0.012709753733202291, 'min_child_samples': 70, 'feature_fraction': 0.9953822181510033, 'bagging_fraction': 0.6832014169448941}. Best is trial#0 with value: 0.0321561742288128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05576140282553469\n",
      "0.056906177909049366\n",
      "0.052155166756764605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-07 22:02:45,099] Finished trial#1 with value: 0.05494091583044955 with parameters: {'n_estimators': 1521, 'num_leaves': 242, 'learning_rate': 0.2980638271480203, 'min_child_samples': 86, 'feature_fraction': 0.6039943519772417, 'bagging_fraction': 0.7722612748995099}. Best is trial#0 with value: 0.0321561742288128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033860309503940755\n"
     ]
    }
   ],
   "source": [
    "training(train, \n",
    "         cv, \n",
    "         model = 'lgbm', \n",
    "         random_state = random_state, \n",
    "         n_trials = 150, \n",
    "         tags = ['catfeatures', 'lgbm catfeatures', 'timestamp from registartion date', \n",
    "                 'all initial features', 'all initial data', 'dropped_nans', 'one model', 'cv ver.2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "neptune": {
   "notebookId": "cdbec88c-05b3-4d8b-ae26-dd3d7490af2d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
